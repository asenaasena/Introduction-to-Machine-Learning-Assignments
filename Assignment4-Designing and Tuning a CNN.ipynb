{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asena to ML - Fall 2020 - Prof. Papernot - Assignment 4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz9MhPEUMrkC"
      },
      "source": [
        "**Assignment**: Designing and Tuning a Convolutional Neural Network (CNN)\n",
        "\n",
        "**Assignment Description**: There are four parts to this assignment\n",
        "\n",
        "1.   Building a CNN\n",
        "2.   Training and Tuning a CNN\n",
        "3.   Trying Out a New Dataset\n",
        "4.   Open-Ended Exploration\n",
        "\n",
        "You will be largely guided through the first two parts. The third and fourth part are discussion based questions. \n",
        "\n",
        "**Before the experiment, make sure that you have GPU enabled. This setting can be found under *Tools --> Settings***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHvVs2GqXxNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6eee3a4-9d8c-4efc-fa58-fe7dc7823255"
      },
      "source": [
        "#Install Objax\n",
        "!pip --quiet install  objax\n",
        "import objax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███████▏                        | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 30kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 40kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25h  Building wheel for objax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqQf8f2RBDcx"
      },
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import jax.numpy as jn\n",
        "import random \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_7vcWRFO39r"
      },
      "source": [
        "##**Part 1. Building a CNN** \n",
        "\n",
        "Before we build our CNN model, let's first import a dataset. For our experiment, we load the CIFAR10 dataset from Tensorflow's dataset repository. The CIFAR10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
        "\n",
        "After loading the dataset, we split the dataset into training, validation and test set. The dataset is originally stored as 50,000 training examples and 10,000 test examples. Instead, we will combine them together and make our own split.\n",
        "\n",
        "Do not change split ratio for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5VQDzs1XodT",
        "outputId": "e4c26d2a-f134-4897-f25a-8cfeda6824dc"
      },
      "source": [
        "#.load_data() by default returns a split between training and test set. \n",
        "# We then adjust the training set into a format that can be accepted by our CNN\n",
        "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "print(X_train.shape)\n",
        "X_train = X_train.transpose(0, 3, 1, 2) / 255.0\n",
        "\n",
        "Y_train = Y_train.flatten()\n",
        "X_test = X_test.transpose(0, 3, 1, 2) / 255.0\n",
        "Y_test = Y_test.flatten()\n",
        "\n",
        "np.random.seed(1)\n",
        "# To create a validation set, we first concate the original splitted dataset into a single dataset \n",
        "# then randomly shuffle the images and labels in the same way (seed = 1)\n",
        "X_data = np.concatenate([X_train, X_test], axis = 0)\n",
        "Y_data = np.concatenate([Y_train, Y_test], axis = 0)\n",
        "\n",
        "N = np.arange(len(X_data))\n",
        "np.random.shuffle(N)\n",
        "X_data = X_data[N]\n",
        "Y_data = Y_data[N]\n",
        "\n",
        "#Next, we partition the randomly shuffled dataset into training, validation and testset according a ratio\n",
        "train_ratio = 0.80\n",
        "valid_ratio = 0.1\n",
        "n_train = int(len(X_data) * train_ratio)\n",
        "n_valid = int(len(X_data) * valid_ratio)\n",
        "\n",
        "X_train, X_valid, X_test = X_data[:n_train], X_data[n_train:n_train+n_valid], X_data[n_train+n_valid:]\n",
        "Y_train, Y_valid, Y_test = Y_data[:n_train], Y_data[n_train:n_train+n_valid], Y_data[n_train+n_valid:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "(50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szDmexFGT7Qs"
      },
      "source": [
        "\n",
        "Next we will construct a **Base Model**, which in our case is a small CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Eeh6jvfBV4p"
      },
      "source": [
        "class ConvNet(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 32, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(32, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "    x = self.conv_2(x)\n",
        "  \n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "model = ConvNet()\n",
        "#print(model.vars())\n",
        "#You can examine the architecture of our CNN by calling model.vars()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crz2dQZBds89"
      },
      "source": [
        "Before we train our conv net, let's try to better understand concepts of convolution filter and linear layer. In the following, you will take the first very image of the training set, create a simple convolution routine, and show that our own routine matches what Objax returns. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Epa7ETf6XddH",
        "outputId": "395b8b83-311e-4616-f91c-86d6399f5fb9"
      },
      "source": [
        "#Let's plot the first image in the training set.\n",
        "plt.imshow(X_train[0].transpose(1,2,0))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ffb2014fed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQjS06vgXZ7r"
      },
      "source": [
        "Next, we will pass our image through Objax's convolution routine. Carefully examine the following code and try to understand the dimension of the filter weights and the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFBbJHBpXXUc",
        "outputId": "78299e46-c334-45a2-c505-14d6a253ca9c"
      },
      "source": [
        "# We append the first image with a batch size of 1 so it can be fed into a convolution layer\n",
        "my_image = np.expand_dims(X_train[0], 0)\n",
        "\n",
        "#Consider a very simple CNN filter with stride = 1 and no padding ('VALID').\n",
        "Conv2d = objax.nn.Conv2D(nin = 3, nout = 2, k = 1, strides = 1, padding = 'VALID', use_bias = False)\n",
        "\n",
        "filter_weights = Conv2d.w.value #This is the initial weight of the filter, which we gradually update when training, we ignore bias for now\n",
        "\n",
        "#print(\"my image:\",np.shape(my_image))\n",
        "print(\"Filter weights:\", filter_weights)\n",
        "print(\"Conv output:\", Conv2d(my_image))\n",
        "print(\"Conv output shape:\", np.shape((filter_weights)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filter weights: [[[[ 0.08896715 -0.07279256]\n",
            "   [ 0.5039943  -0.5607323 ]\n",
            "   [ 0.69289994 -0.86930597]]]]\n",
            "Conv output: [[[[ 0.14117865  0.13613605  0.11829102 ...  1.0097966   0.9931872\n",
            "     0.98468655]\n",
            "   [ 0.19160458  0.16441517  0.16871695 ...  1.0254501   1.0205964\n",
            "     1.0155538 ]\n",
            "   [ 0.19664718  0.16639161  0.19392993 ...  1.0188706   1.0191027\n",
            "     1.0148438 ]\n",
            "   ...\n",
            "   [ 0.22187413  0.21178894  0.21178894 ...  0.27654663  0.28158924\n",
            "     0.29671702]\n",
            "   [ 0.18657598  0.17649078  0.17649078 ...  0.3068022   0.30175963\n",
            "     0.30175963]\n",
            "   [ 0.1664056   0.161363    0.1664056  ...  0.37739852  0.37235594\n",
            "     0.35722816]]\n",
            "\n",
            "  [[-0.16435704 -0.15846358 -0.13737418 ... -1.1958665  -1.1757659\n",
            "    -1.1652533 ]\n",
            "   [-0.22329159 -0.19162536 -0.19630872 ... -1.2115611  -1.2058\n",
            "    -1.1999066 ]\n",
            "   [-0.22918504 -0.1938243  -0.225776   ... -1.2002045  -1.2009761\n",
            "    -1.196932  ]\n",
            "   ...\n",
            "   [-0.259312   -0.24752508 -0.24752508 ... -0.31988704 -0.32578048\n",
            "    -0.34346086]\n",
            "   [-0.21805783 -0.20627092 -0.20627092 ... -0.35524777 -0.3493543\n",
            "    -0.3493543 ]\n",
            "   [-0.19448401 -0.18859056 -0.19448401 ... -0.43775612 -0.43186268\n",
            "    -0.4141823 ]]]]\n",
            "Conv output shape: (1, 1, 3, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsDg8yateSuH"
      },
      "source": [
        "**In the cells below, you will create your own convolution routine that takes in the image and the initial weights used by Objax's own convolution routine (Conv2d.w.value) and show that your convolution routine returns the same value than Objax's.**\n",
        "\n",
        "A simple implementation only requires 4 FOR loops. You may wish to draw inspiration from https://objax.readthedocs.io/en/latest/objax/nn.html?highlight=objax.nn.Conv2D#objax.nn.Conv2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5B6K9gyXTAx",
        "outputId": "5c726171-d3fb-494d-c1f0-6d8f50217047"
      },
      "source": [
        "#Solution to the problem PART1.2\n",
        "conv_output=Conv2d(my_image)\n",
        "\n",
        "#This is a function for more than 1 number of examples and filter size>1\n",
        "def my_conv_net(my_image, initial_filter_weights):\n",
        "  my_conv_output=np.zeros([1,2,32,32])\n",
        "  stride=1\n",
        "  filter_size_x=1\n",
        "  filter_size_y=1\n",
        "  w=((32-filter_size_x)/stride)+1\n",
        "  h=((32-filter_size_y)/stride)+1\n",
        "  for i in range(1): # m # loop over the batch of training examples\n",
        "     for h in range(32):     #n_H                      # loop over vertical axis of the output volume\n",
        "            for w in range(32):  #n_W                     # loop over horizontal axis of the output volume\n",
        "                for c in range(2):   #n_C                # loop over channels (= #filters) of the output volume\n",
        "                    \n",
        "                    # Finding the corners of the current \"slice\" \n",
        "                    vert_start  = h * stride\n",
        "                    vert_end    = vert_start + filter_size_y\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end   = horiz_start + filter_size_x\n",
        "                    \n",
        "                    \n",
        "                    s =np.multiply(my_image[i,:,vert_start:vert_end,horiz_start:horiz_end,].flatten(),(initial_filter_weights[:,:,:,c]).flatten() )\n",
        "                    my_conv_output[i,c, h, w]  = my_conv_output[i,c, h, w]+np.sum(s)\n",
        "                 \n",
        "\n",
        "  return my_conv_output\n",
        "\n",
        "my_conv_output=my_conv_net(my_image,filter_weights)\n",
        "print(\"Convolution output manually computed :\", my_conv_output)\n",
        "\n",
        "#To see that they are equal\n",
        "#There is just very little difference in the least significant digit\n",
        "# that's why, np.equal gives wrong result even though the results are basically same\n",
        "print(my_conv_output.flatten())\n",
        "print(conv_output.flatten())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Convolution output manually computed : [[[[ 0.14117865  0.13613605  0.11829102 ...  1.00979659  0.9931872\n",
            "     0.98468654]\n",
            "   [ 0.19160458  0.16441517  0.16871695 ...  1.02545006  1.02059643\n",
            "     1.01555384]\n",
            "   [ 0.19664718  0.16639161  0.19392992 ...  1.01887065  1.01910264\n",
            "     1.01484388]\n",
            "   ...\n",
            "   [ 0.22187412  0.21178893  0.21178893 ...  0.27654664  0.28158923\n",
            "     0.29671702]\n",
            "   [ 0.18657596  0.17649078  0.17649078 ...  0.3068022   0.30175961\n",
            "     0.30175961]\n",
            "   [ 0.16640559  0.161363    0.16640559 ...  0.37739851  0.37235592\n",
            "     0.35722814]]\n",
            "\n",
            "  [[-0.16435704 -0.15846358 -0.13737418 ... -1.19586641 -1.17576586\n",
            "    -1.16525327]\n",
            "   [-0.22329158 -0.19162536 -0.19630872 ... -1.21156101 -1.20580003\n",
            "    -1.19990658]\n",
            "   [-0.22918503 -0.19382431 -0.22577599 ... -1.20020451 -1.20097615\n",
            "    -1.19693196]\n",
            "   ...\n",
            "   [-0.25931199 -0.24752508 -0.24752508 ... -0.31988702 -0.32578048\n",
            "    -0.34346084]\n",
            "   [-0.21805781 -0.2062709  -0.2062709  ... -0.35524775 -0.34935429\n",
            "    -0.34935429]\n",
            "   [-0.19448399 -0.18859054 -0.19448399 ... -0.43775611 -0.43186265\n",
            "    -0.41418229]]]]\n",
            "[ 0.14117865  0.13613605  0.11829102 ... -0.43775611 -0.43186265\n",
            " -0.41418229]\n",
            "[ 0.14117865  0.13613605  0.11829102 ... -0.43775612 -0.43186268\n",
            " -0.4141823 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9dbovOwiVDE"
      },
      "source": [
        "The outputs of last convolution layer is typically rearranged so it can be fed into a linear layer. Check that calling .mean((2,3)) rearranges the output of your convolution routine by examining the shape of the output. (Not graded) Think about alternative ways of rearranging the output from the convolution layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7hngIUNXIMQ",
        "outputId": "04db5de2-8222-4366-ffe3-317344d419a7"
      },
      "source": [
        "#Check that .mean((2,3)) rearranges your image\n",
        "arr=my_conv_output.mean((2,3))\n",
        "\n",
        "print(arr)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.35491109 -0.41205534]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTWfvL0mis3D"
      },
      "source": [
        "Take your rearranged output and feed it into a linear layer of appropriate size. Here is an example:\n",
        "\n",
        "```\n",
        "Linear_Layer = objax.nn.Linear(N, 1)\n",
        "Y = Linear_Layer(X)\n",
        "```\n",
        "Next, extract the weights and bias of the linear layer using \n",
        "```\n",
        "Linear_Layer.w.value\n",
        "Linear_Layer.b.value\n",
        "```\n",
        "**Using these values, write one line of code that manually implements the linear layer. Show that it provides the same value as Objax's own linear layer.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq-TkFpgXDC7",
        "outputId": "b1b7d735-be4b-4bde-8997-cceaa7f02c04"
      },
      "source": [
        "#print(\"arr:\",arr) #Part 1.3\n",
        "Linear_Layer = objax.nn.Linear(2, 1)\n",
        "Y = Linear_Layer(arr)\n",
        "\n",
        "\n",
        "linear_w=Linear_Layer.w.value\n",
        "print(\"Linear Layer w value:\",linear_w)\n",
        "linear_b=Linear_Layer.b.value\n",
        "print(\"Linear Layer b value:\",linear_b)\n",
        "\n",
        "#This is the part we computed Y that manually implements linear layer\n",
        "manual_y=sum(arr.flatten()*linear_w.flatten()+linear_b)\n",
        "\n",
        "#To check the results\n",
        "print(\"manually calculated y:\",manual_y)\n",
        "print(\"Y:\",Y)\n",
        "print(\"Y values are equal\") #again only least significant digits differs, but\n",
        "#this is just related how it computes the values not methodologic difference\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Layer w value: [[-0.17576653]\n",
            " [-0.33976567]]\n",
            "Linear Layer b value: [0.]\n",
            "manually calculated y: 0.07762077450752258\n",
            "Y: [[0.07762077]]\n",
            "Y values are equal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96QQXl3d2kZn"
      },
      "source": [
        "You have now completed Part 1 of the assignment. Good job!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kqaH75UUaSE"
      },
      "source": [
        "##**Part 2. Training and Tuning a CNN**\n",
        "\n",
        "The following starter code trains the neural network in Part 1. However, the optimizer and batch sampling routine are left for you to implement. Complete the lines that says #PUT YOUR CODE HERE#\n",
        "\n",
        "Afterwards, train the model, and observe the training/validation loss and accuracy plots. You should observe that the validation accuracy is low and stagnates after a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBcHWoCl0URZ"
      },
      "source": [
        "#Define loss function as averaged value of of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = model(x)\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(model(x)), model.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, model.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in model.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    count=0\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      params.value -= lr * grad\n",
        "      \n",
        "      ####################\n",
        "    \n",
        "        \n",
        "      \n",
        "      #PUT YOUR CODE HERE#\n",
        "      ####################                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGPpVTfG0Ug1"
      },
      "source": [
        "def train(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss =  0  # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0        # validation accuracy per batch\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          batch =train_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "      # run validation\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "          batch = val_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))\n",
        "  \n",
        "  #Plot training loss\n",
        "  plt.title(\"Train vs Validation Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train vs Validation Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8YqWtV5VYW45",
        "outputId": "6684f57c-3848-4daa-d273-266d07e69fab"
      },
      "source": [
        "\n",
        "train(EPOCHS = 60, BATCH = 32, LEARNING_RATE = 9e-4) #Part 2 base model with epochs=60"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0001  Training Loss 2.30 Validation Loss 2.30 Training Accuracy 9.88 Validation Accuracy 10.03\n",
            "Epoch 0002  Training Loss 2.30 Validation Loss 2.29 Training Accuracy 9.78 Validation Accuracy 10.02\n",
            "Epoch 0003  Training Loss 2.29 Validation Loss 2.29 Training Accuracy 9.90 Validation Accuracy 10.32\n",
            "Epoch 0004  Training Loss 2.29 Validation Loss 2.29 Training Accuracy 10.52 Validation Accuracy 10.58\n",
            "Epoch 0005  Training Loss 2.29 Validation Loss 2.29 Training Accuracy 10.49 Validation Accuracy 11.25\n",
            "Epoch 0006  Training Loss 2.29 Validation Loss 2.28 Training Accuracy 11.10 Validation Accuracy 10.77\n",
            "Epoch 0007  Training Loss 2.28 Validation Loss 2.28 Training Accuracy 11.10 Validation Accuracy 11.62\n",
            "Epoch 0008  Training Loss 2.28 Validation Loss 2.28 Training Accuracy 11.50 Validation Accuracy 12.08\n",
            "Epoch 0009  Training Loss 2.28 Validation Loss 2.27 Training Accuracy 11.84 Validation Accuracy 12.25\n",
            "Epoch 0010  Training Loss 2.27 Validation Loss 2.27 Training Accuracy 12.33 Validation Accuracy 12.58\n",
            "Epoch 0011  Training Loss 2.27 Validation Loss 2.27 Training Accuracy 12.70 Validation Accuracy 12.90\n",
            "Epoch 0012  Training Loss 2.26 Validation Loss 2.26 Training Accuracy 12.94 Validation Accuracy 13.13\n",
            "Epoch 0013  Training Loss 2.26 Validation Loss 2.26 Training Accuracy 13.40 Validation Accuracy 13.50\n",
            "Epoch 0014  Training Loss 2.25 Validation Loss 2.25 Training Accuracy 13.62 Validation Accuracy 13.85\n",
            "Epoch 0015  Training Loss 2.25 Validation Loss 2.24 Training Accuracy 14.31 Validation Accuracy 14.37\n",
            "Epoch 0016  Training Loss 2.24 Validation Loss 2.24 Training Accuracy 14.90 Validation Accuracy 14.60\n",
            "Epoch 0017  Training Loss 2.24 Validation Loss 2.23 Training Accuracy 15.56 Validation Accuracy 15.55\n",
            "Epoch 0018  Training Loss 2.23 Validation Loss 2.22 Training Accuracy 16.02 Validation Accuracy 16.88\n",
            "Epoch 0019  Training Loss 2.22 Validation Loss 2.21 Training Accuracy 16.91 Validation Accuracy 17.40\n",
            "Epoch 0020  Training Loss 2.21 Validation Loss 2.21 Training Accuracy 17.56 Validation Accuracy 18.27\n",
            "Epoch 0021  Training Loss 2.20 Validation Loss 2.20 Training Accuracy 18.29 Validation Accuracy 18.23\n",
            "Epoch 0022  Training Loss 2.20 Validation Loss 2.19 Training Accuracy 18.60 Validation Accuracy 19.38\n",
            "Epoch 0023  Training Loss 2.19 Validation Loss 2.18 Training Accuracy 19.02 Validation Accuracy 19.68\n",
            "Epoch 0024  Training Loss 2.18 Validation Loss 2.17 Training Accuracy 19.54 Validation Accuracy 20.20\n",
            "Epoch 0025  Training Loss 2.17 Validation Loss 2.16 Training Accuracy 19.79 Validation Accuracy 20.37\n",
            "Epoch 0026  Training Loss 2.16 Validation Loss 2.15 Training Accuracy 20.24 Validation Accuracy 20.33\n",
            "Epoch 0027  Training Loss 2.15 Validation Loss 2.14 Training Accuracy 20.40 Validation Accuracy 20.20\n",
            "Epoch 0028  Training Loss 2.14 Validation Loss 2.13 Training Accuracy 20.35 Validation Accuracy 20.58\n",
            "Epoch 0029  Training Loss 2.14 Validation Loss 2.13 Training Accuracy 20.69 Validation Accuracy 20.72\n",
            "Epoch 0030  Training Loss 2.13 Validation Loss 2.12 Training Accuracy 21.06 Validation Accuracy 20.92\n",
            "Epoch 0031  Training Loss 2.12 Validation Loss 2.11 Training Accuracy 21.33 Validation Accuracy 20.97\n",
            "Epoch 0032  Training Loss 2.12 Validation Loss 2.11 Training Accuracy 21.58 Validation Accuracy 21.78\n",
            "Epoch 0033  Training Loss 2.11 Validation Loss 2.11 Training Accuracy 21.82 Validation Accuracy 22.05\n",
            "Epoch 0034  Training Loss 2.11 Validation Loss 2.10 Training Accuracy 22.18 Validation Accuracy 22.03\n",
            "Epoch 0035  Training Loss 2.11 Validation Loss 2.10 Training Accuracy 22.07 Validation Accuracy 22.08\n",
            "Epoch 0036  Training Loss 2.10 Validation Loss 2.10 Training Accuracy 22.31 Validation Accuracy 22.27\n",
            "Epoch 0037  Training Loss 2.10 Validation Loss 2.09 Training Accuracy 22.35 Validation Accuracy 22.22\n",
            "Epoch 0038  Training Loss 2.10 Validation Loss 2.09 Training Accuracy 22.65 Validation Accuracy 21.93\n",
            "Epoch 0039  Training Loss 2.10 Validation Loss 2.09 Training Accuracy 22.30 Validation Accuracy 22.20\n",
            "Epoch 0040  Training Loss 2.09 Validation Loss 2.09 Training Accuracy 22.69 Validation Accuracy 22.38\n",
            "Epoch 0041  Training Loss 2.09 Validation Loss 2.09 Training Accuracy 22.88 Validation Accuracy 22.58\n",
            "Epoch 0042  Training Loss 2.09 Validation Loss 2.09 Training Accuracy 23.04 Validation Accuracy 22.57\n",
            "Epoch 0043  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.09 Validation Accuracy 22.83\n",
            "Epoch 0044  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.00 Validation Accuracy 22.50\n",
            "Epoch 0045  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.21 Validation Accuracy 23.45\n",
            "Epoch 0046  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.35 Validation Accuracy 23.02\n",
            "Epoch 0047  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.43 Validation Accuracy 22.47\n",
            "Epoch 0048  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.37 Validation Accuracy 23.18\n",
            "Epoch 0049  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.53 Validation Accuracy 23.03\n",
            "Epoch 0050  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.40 Validation Accuracy 23.55\n",
            "Epoch 0051  Training Loss 2.09 Validation Loss 2.08 Training Accuracy 23.79 Validation Accuracy 23.02\n",
            "Epoch 0052  Training Loss 2.08 Validation Loss 2.08 Training Accuracy 23.59 Validation Accuracy 23.90\n",
            "Epoch 0053  Training Loss 2.08 Validation Loss 2.08 Training Accuracy 24.00 Validation Accuracy 23.27\n",
            "Epoch 0054  Training Loss 2.08 Validation Loss 2.08 Training Accuracy 23.94 Validation Accuracy 22.97\n",
            "Epoch 0055  Training Loss 2.08 Validation Loss 2.08 Training Accuracy 23.87 Validation Accuracy 23.38\n",
            "Epoch 0056  Training Loss 2.08 Validation Loss 2.08 Training Accuracy 24.08 Validation Accuracy 24.13\n",
            "Epoch 0057  Training Loss 2.08 Validation Loss 2.07 Training Accuracy 24.06 Validation Accuracy 24.18\n",
            "Epoch 0058  Training Loss 2.08 Validation Loss 2.07 Training Accuracy 24.51 Validation Accuracy 23.53\n",
            "Epoch 0059  Training Loss 2.08 Validation Loss 2.07 Training Accuracy 24.51 Validation Accuracy 23.27\n",
            "Epoch 0060  Training Loss 2.08 Validation Loss 2.07 Training Accuracy 24.24 Validation Accuracy 24.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gU1frA8e+b3hNCQgslIE1aKAFEqSoKiqAUBRv8uMqFawEbduz3XhVRUKyA7aooUqyoiFRBIGDoHQKEGkhCAunJ+f0xEwxxCQlksynv53nm2d2ZM7PvLGHfPefMnCPGGJRSSqnC3FwdgFJKqfJJE4RSSimHNEEopZRySBOEUkophzRBKKWUckgThFJKKYc0QahyS0Tmi8hwV8dxIUTkIxF50X7eTUS2F6fsBb7XKRFpdKH7K3UumiBUqbK/rPKXPBFJL/D6tpIcyxjT1xjzsbNiLYqIDBWROBGRQus9ROSYiPQr7rGMMcuMMc1KKa7FInJXoeMHGGP2lMbxC71XnIhcXdrHVRWHJghVquwvqwBjTACwH7ihwLrP8suJiIfroiyWeUAI0KPQ+j6AAX4q84iUKmOaIFSZEJGeIhIvIo+KyBHgQxGpJiLfi0iCiCTZz+sW2OfMr2URGSEiy0Vkol12r4j0Pcd7PSoiXxdaN1lEphQ41h4RSbWP87eajTEmA/gKuLPQpjuBz40xOSIyS0SOiMhJEVkqIi2LOvcCr9uJyDr7/b8EfApsO+dnIiIvAd2At+wa2Vv2eiMije3nwSLyib3/PhF5SkTcSvoZFkVEvEXkDRE5ZC9viIi3vS3MjjlZRBJFZFmB939URA7a571dRK4q6XursqUJQpWlWkAo0AAYhfX396H9uj6QDrxVxP6dge1AGPAKML1wE5BtJnCdiAQCiIg7cDPwuYj4A1OAvsaYQOByIPYc7/cxMFhEfO3jBAM32OsB5gNNgBrAOuAzRwcpSES8sGonn2J9FrOAQQWKnPMzMcY8CSwD7rVrZPc6eIs3gWCgEVbt507g/wpsL+5nWJQngcuAtkAU0Al4yt72EBAPhAM1gScAIyLNgHuBjvbnfi0QV8L3VWVME4QqS3nAM8aYTGNMujHmhDFmtjEmzRiTCrzE35t0CtpnjPnAGJOL9SVdG+tL6CzGmH1YX9g32auuBNKMMX8UiKOViPgaYw4bYzY7ejNjzO/A0QLHuRnYYYyJtbfPMMakGmMygWeBKDuJFOUywBN4wxiTbYz5GlhT4D1L+pmcYSfCocDjdlxxwGvAHQWKFeszPI/bgOeNMceMMQnAcwXeI9s+ZgP7/JYZa8C3XMAbaCEinsaYOGPM7hK+rypjmiBUWUqwm24AEBE/EXnPbgpJAZYCIfYXnSNH8p8YY9LspwHnKPs5MMx+fqv9GmPMaeAWYDRwWER+EJHmRcT8CX81M91hv0ZE3EXkvyKy2449zi4TVsSxAOoAB83Zo2Tuy39yAZ9JQWFYyWdfgXX7gIgCr0vyGRZ1DoXfo479/FVgF/CL3Yz3mP1eu4BxWIn0mIjMFJE6qHJNE4QqS4WHDn4IaAZ0NsYEAd3t9SVt8nBkFtDTbr+/CTtBABhjfjbG9Mb6pbsN+KCI43wKXCUiXbB+/ec3I90KDACuxmrSiSxm7IeBiELNOvULPD/fZ1LU8MvHsX7BNyh07IPniamkDjl4j0MAds3lIWNMI6A/8GB+X4Mx5nNjTFd7XwO8XMpxqVKmCUK5UiBWG3uyiIQCz5TWge2mj8VY7fl7jTFbAUSkpogMsPsiMoFTWE1O5zpOHLAc+AJYYIzJ/wUeaO9/AvAD/l3M0FYCOcD9IuIpIgOx2vDzne8zOYrVv+Ao1lysjvWXRCRQRBoADwL/K2ZsjniKiE+BxQPrs3hKRMJFJAyYkP8eItJPRBrbCfAkVtNSnog0E5Er7c7sDPscz/m5q/JBE4RypTcAX6xfvn9Q+peOfo71C//zAuvcsL40DwGJWO37Y85znI+xfvV+UmDdJ1hNKweBLVjxn5cxJgsYCIyw3/8WYE6BIuf7TCZjdZwn5V+VVch9wGlgD1Zi+xyYUZzYzuFHrC/z/OVZ4EUgBtgAbMTq78m/0a8J8CtW4l0JvG2MWYTV//Bf+7yOYHXsP34RcakyIDphkFJKKUe0BqGUUsohTRBKKaUc0gShlFLKIU0QSimlHCrvA6aVSFhYmImMjHR1GEopVWGsXbv2uDEm3NG2SpUgIiMjiYmJcXUYSilVYYjIvnNt0yYmpZRSDmmCUEop5ZAmCKWUUg5Vqj4IpVTlkZ2dTXx8PBkZGecvrM7Lx8eHunXr4unpWex9NEEopcql+Ph4AgMDiYyMpORzGqmCjDGcOHGC+Ph4GjZsWOz9tIlJKVUuZWRkUL16dU0OpUBEqF69eolrY5oglFLlliaH0nMhn6XTEoSI1BORRSKyRUQ2i8hYB2UGiMgGEYkVkRgR6Vpg23AR2Wkvw50VpzGGNxfuZNPBk856C6WUqpCcWYPIAR4yxrTAmonrHhFpUajMQiDKGNMWGAlMAygwUUpnrMlUnhGRas4I8mR6Nl+s3s/wGavZk3DKGW+hlKqATpw4Qdu2bWnbti21atUiIiLizOusrKwi942JieH+++8vo0idx2kJwp4Mfp39PBXYytlz42KMOVVgbl5//ppO8Vqs2bsSjTFJwAKgjzPiDPHz4tO7OgNwx/TVHEpOd8bbKKUqmOrVqxMbG0tsbCyjR4/mgQceOPPay8uLnJycc+4bHR3NlCmO5nOqWMqkD0JEIoF2wCoH224SkW3AD1i1CLASyYECxeIplFxK0yXhAXw8shMp6dncMX0VJ05lOuutlFIV2IgRIxg9ejSdO3dm/PjxrF69mi5dutCuXTsuv/xytm/fDsDixYvp168fAM8++ywjR46kZ8+eNGrUqEIlDqdf5ioiAcBsYJwxJqXwdmPMXGCuiHQHXsCaIrIkxx8FjAKoX7/+eUqfW6uIYKYNj+bOGasZ8eEaPr+7M4E+xb9eWCnlPM99t5kth/729XFRWtQJ4pkbWpZ4v/j4eFasWIG7uzspKSksW7YMDw8Pfv31V5544glmz579t322bdvGokWLSE1NpVmzZowZM6ZE9yO4ilNrECLiiZUcPjPGzCmqrDFmKdDIngT9IFCvwOa69jpH+71vjIk2xkSHhzsckPD8DqyB7Aw6N6rOO7e3Z+vhFO76OIaM7NwLO55SqtIaMmQI7u7uAJw8eZIhQ4bQqlUrHnjgATZv3uxwn+uvvx5vb2/CwsKoUaMGR48eLcuQL5jTahBiXVM1HdhqjJl0jjKNgd3GGCMi7bEmNj8B/Az8u0DH9DU4a4LzjJPw6Y3gFQBXjOXKDiN47eYoxn0Zy5B3V/LQNU3p0TRcL7dTyoUu5Je+s/j7+595/vTTT9OrVy/mzp1LXFwcPXv2dLiPt7f3mefu7u5F9l+UJ86sQVwB3AFcaV/GGisi14nIaBEZbZcZBGwSkVhgKnCLsSRiNTetsZfn7XWlzzsIhn0BYU3g58dhchsGnJ7N20Oakng6ixEfrmHgOytYsiOBv/rTlVLKqkFERFjdox999JFrg3ECp9UgjDHLgSJ/dhtjXgZePse2GcAMJ4R2NhFo2N1a9q2AJa/Agqfp6/s6vTuPYZ5HH17//TjDZ6ymXf0Q7r+qCT21RqGUAsaPH8/w4cN58cUXuf76610dTqmTyvSrODo62pTKhEEH1sDSV2DnL+DpR27UbXzvfxMv/5HBoZMZNK0ZwF1dGzGgXR28Pdwv/v2UUn+zdetWLr30UleHUak4+kxFZK0xJtpReR1qw5F6HeG2WTBmJbQciPu6jxiwtB/LG33Mh1fl4QaMn72Bri8v4q3fdpJ0uuibZpRSqiLSBFGUmi3gxqkwbiNcMQ63vYvp9fvtzPd+jIVdtxFdU5j4yw66/HchT8zdyK5jqa6OWCmlSo0O910cQbXh6meg20OwaTay9iMuiXmedzx8ONmmH5/lXMkba+HzVfvp3jSckVdE0r1JOG5u2k+hlKq4NEGUhHcAdBhuLYfXw9qPCd7wFf/K+pq7G3RmfrVbeXGbFyM+XMMl4f6M7NqQQe3r4uOp/RRKqYpHm5guVO0o6DcJHt4OfV/FM/Ug/TeN5Y/qz/FV18MEeApPzt3E5f/9jUkLdpCQqsN3KKUqFk0QF8vLHzqPgvv/hAFv45aTQaeYh5jHA/zWaz8d6wXw5m87ueK/vzH+6/XsOqYjxiqlKgZNEKXFwwva3Qb3rIKbP0G8/Gm08jHeS7qbNb33cmuHcL5df4jery9hzP/WsjFe559Qqjzr1asXP//881nr3njjDcaMGeOwfM+ePcm/zP66664jOTn5b2WeffZZJk6cWOT7zps3jy1btpx5PWHCBH799deShl8qNEGUNjd3aDEA/rkUbvsagiIIW/okz+4extpeWxjXrTbLdx3nhreWc8f0Vfyx54Teoa1UOTRs2DBmzpx51rqZM2cybNiw8+77448/EhISckHvWzhBPP/881x9dYnGMC01miCcRQSa9IaRP8GIH6BmS/yXPs/YDTexputanr6qNlsPpzD0/T+45b0/+GPPCVdHrJQqYPDgwfzwww9nJgeKi4vj0KFDfPHFF0RHR9OyZUueeeYZh/tGRkZy/PhxAF566SWaNm1K165dzwwHDvDBBx/QsWNHoqKiGDRoEGlpaaxYsYJvv/2WRx55hLZt27J7925GjBjB119/DcDChQtp164drVu3ZuTIkWRmZp55v2eeeYb27dvTunVrtm3bViqfgV7F5GwiENnVWuLXwrKJ+Cx/mX94vc3wTv/ga8/+TFqRxND3/+DyS6rzYO+mREeGujpqpcqX+Y/BkY2le8xaraHvf8+5OTQ0lE6dOjF//nwGDBjAzJkzufnmm3niiScIDQ0lNzeXq666ig0bNtCmTRuHx1i7di0zZ84kNjaWnJwc2rdvT4cOHQAYOHAgd999NwBPPfUU06dP57777qN///7069ePwYMHn3WsjIwMRowYwcKFC2natCl33nkn77zzDuPGjQMgLCyMdevW8fbbbzNx4kSmTZt20R+R1iDKUt0O1sCAo3+HJr3xWDGZob9fz4r2C3npmlrsOJrK4HdXcueM1aw/8Pf2S6VU2SrYzJTfvPTVV1/Rvn172rVrx+bNm89qDips2bJl3HTTTfj5+REUFET//v3PbNu0aRPdunWjdevWfPbZZ+ccKjzf9u3badiwIU2bNgVg+PDhLF269Mz2gQMHAtChQwfi4uIu9JTPojUIV6jVCoZ8CL2egGWv4bH6XW7z+JibO93N5+4DmLziBAOm/s71rWvz8LXNaBjmf/5jKlWZFfFL35kGDBjAAw88wLp160hLSyM0NJSJEyeyZs0aqlWrxogRI8jIyLigY48YMYJ58+YRFRXFRx99xOLFiy8q1vwhxUtzOHGtQbhSWBO46V24ZzU064PniskMXz2AlV3W8HCP2izafozek5bw9LxNeh+FUi4QEBBAr169GDlyJMOGDSMlJQV/f3+Cg4M5evQo8+fPL3L/7t27M2/ePNLT00lNTeW77747sy01NZXatWuTnZ3NZ599dmZ9YGAgqal/H7anWbNmxMXFsWvXLgA+/fRTevToUUpn6pgmiPIgrAkMngFjfoeG3fFe/jL3brqFldcfZ1jHenyxej89Xl3E5F936ix3SpWxYcOGsX79eoYNG0ZUVBTt2rWjefPm3HrrrVxxxRVF7tu+fXtuueUWoqKi6Nu3Lx07djyz7YUXXqBz585cccUVNG/e/Mz6oUOH8uqrr9KuXTt27959Zr2Pjw8ffvghQ4YMoXXr1ri5uTF69GicSYf7Lo/i18KPD8OhddDgCuIvf4F/x8CPG48QEeLL0/0u5dqWtXROClWp6XDfpU+H+64M6naAuxbCDZPh2BbqzuzN22Fz+HJEKwJ9PBj9v3XcMX01O4/q6LFKKefRBFFeublBhxFw71podzusfIvOP1zLD1ed4LkbWrAhPpm+k5fx4vdbOJ1ZMea3VUpVLJogyjv/6tB/ilWj8A/DffYIhu99hCV3RTIkui7Tlu/l2jeWsmxngqsjVarUVaYmcFe7kM9SE0RFUTca7l4Mff4L+1dS7aPu/CfsF2bd1QEvdzfumL6ah2etJzlNZ7dTlYOPjw8nTuhQNKXBGMOJEyfw8fEp0X7aSV0RpRyCnx6DLd9AeHMy+73F5K2BvLd0D9X8vHhhQEv6tq7t6iiVuijZ2dnEx8df8H0G6mw+Pj7UrVsXT0/Ps9YX1UmtCaIi2/ELfD8OUo9A13FsbjKa8fO2s/lQCgPbRfDcgJYE+nie/zhKqSpLr2KqrJpeA2NWQNRQWPYaLb+/kW8GBjL2qibMiz3IdVOWsXZfkqujVEpVUJogKjrfELjxbRj2JaQdx2PGVTzgOZev7u6EMXDzeyt549cd5OTmuTpSpVQFowmismjWB/71B7S8CRb/m+jldzH/7hb0j6rDG7/u5Jb3/+Bgcrqro1RKVSCaICoTv1AYNA36vwn7VhD4cW9e7waTh7Zl+5FU+k1ZxvKdx10dpVKqgtAEURm1v9OaqMjkwYxrGcASvr33CsIDvblzxiqmLtpFXl7luThBKeUcmiAqq4gO8M8lUK8TzBtDo9XPMPefHenXpg6v/rydUZ/GcDI929VRKqXKMU0QlZl/GNw+Fy6/H9ZMw3/mICbfEMEzN7Rg8fYE+r+1nB06npNS6hw0QVR27h5wzQswaDoc+hP54Er+r1EKM0ddRlpWLoPfWUFMXKKro1RKlUOaIKqK1oPtfgkD068l+tQi5oy5nLAAb26btooFW466OkKlVDmjCaIqqdMWRi2yHr8eSb11rzLrn51pXjuIf34aw8zV+10doVKqHNEEUdUE1IA7v7WGEl8+ieq/3M/nI6Pp1iScx+ZsZMrCnTo4mlIKAA9XB6BcwMML+r0BwfXgtxfwF2HaHVN5dO5mJi3YQeLpLJ65oYXOWKdUFee0BCEi9YBPgJqAAd43xkwuVOY24FFAgFRgjDFmvb0tzl6XC+ScazApdYFEoPvDgIHfXsQT4bVBU6nm58X05XvJys3jxQGtcHPTJKFUVeXMGkQO8JAxZp2IBAJrRWSBMWZLgTJ7gR7GmCQR6Qu8D3QusL2XMUZv/XWm7o9Y6XvRi4gIT/V/Cy8PN95ZvJuc3Dz+M7AN7poklKqSnJYgjDGHgcP281QR2QpEAFsKlFlRYJc/gLrOikcVoccjgIFFLyHixvgbpuDp7saUhTvJyTW8OiRKk4RSVVCZ9EGISCTQDlhVRLF/APMLvDbALyJigPeMMe87LUAFPcZbQ3Ms/g+C8GD/N/F0E15bsIPsPMPrN0fh4a7XNChVlTg9QYhIADAbGGeMSTlHmV5YCaJrgdVdjTEHRaQGsEBEthljljrYdxQwCqB+/fqlHn+V0vMx6z6JJf8F4L7+b+Lh7sbLP20jL88weWhbTRJKVSFOTRAi4omVHD4zxsw5R5k2wDSgrzHmRP56Y8xB+/GYiMwFOgF/SxB2zeJ9sGaUK/WTqGp6PW492kliTP838XATXvpxK94ebkwcEqUd10pVEc68ikmA6cBWY8ykc5SpD8wB7jDG7Ciw3h9ws/su/IFrgOedFasqpFCSuLv/m2Tm5DLxlx14e7rz75ta6SWwSlUBzqxBXAHcAWwUkVh73RNAfQBjzLvABKA68Lb9hZN/OWtNYK69zgP43BjzkxNjVYUVShL39n+TtKxc3l68Gx9PNyb00/sklKrsnHkV03Ks+xuKKnMXcJeD9XuAKCeFpoqrYJIQ4ZEbppCencuHv8fh5+XOI9c2d218Simn0jupVdF6PQ4mF5a+iviHMaHfM2Rk5zF10W58Pd2598omro5QKeUkmiDU+fV6Ek4fh+WvI/41eOnGMWRkW30SYQHeDO2kV48pVRlpglDnJwLXvwZpx+Hnx3HzD+OVwYNJPJ3Fk/M2USPImyub13R1lEqpUqYXtavicXOHgdMgshvMG4Pn3t94+7b2XFo7kHs++5P1B5JdHaFSqpRpglDF5+kDQz+D8EvhyzvxT1jPjBEdqR7gxciP1hB3/LSrI1RKlSJNEKpkfILh9tnWfNefD6FGzlE+HtmJPGMY8eFqTpzKdHWESqlSoglClVxgTbh9DuTmwMxbuSQIpg3vyOGTGYz8OIb0rFxXR6iUKgWaINSFCWsMQ2bAsS0w9590qBfMlGHt2BCfzCNfr9dZ6ZSqBDRBqAvX+Gq45kXY9j0seZlrW9Zi/LXN+X7DYd78bZero1NKXSS9zFVdnMv+BUc3W3db12zB6B792Xk0lUkLdtCkRgB9W9d2dYRKqQukCUJdHBHo9zoc3wFzRyOhjfj3wNbsPXGaB76KpV6oH60igl0dpVLqAmgTk7p4Ht5wy//AJwS+GIZPVjLv3dGBUD8v7v4khmOpGa6OUCl1ATRBqNIRWMu6R+LUUZhzFzX8PflgeDTJadmM+mQtGdl6ZZNSFY0mCFV6ItrDda/C7t9gycu0rBPM67dEEXsgmee+2+zq6JRSJaQJQpWu9sOh7e2w5GXY8Qt9WtXmXz0v4YvVB/hqzQFXR6eUKgFNEKp0icD1E6Fma5hzNyTF8dA1zejaOIynvtnEpoMnXR2hUqqYNEGo0ufpC7d8AsbAV3finpvJ5KFtCfP3YvT/1pKcluXqCJVSxaAJQjlHaCMY+B4cXg/zx1M9wJupt7XnaEoG476MJS9P77RWqrzTBKGcp1lf6PYQrPsYYj+nXf1qTLihJYu3JzDlt52ujk4pdR6aIJRz9XrSmkPi+wfh2FZu71yfge0jmLxwJ0t2JLg6OqVUETRBKOdyc4dB08A7EL4ajmSd5qUbW9O0RiAPfhnL0RS9iU6p8koThHK+wFow6ANrOI4fHsLX0423bm1HWlYu42bGkqv9EUqVS5ogVNlo1BN6PgYbZsKfn9KkZiDPDWjJyj0nmLpIR35VqjzSBKHKTvdHoGEP+PEROLKJIR3qcmPbOrzx6w5W7Tnh6uiUUoVoglBlJ78/wicYZg1Hsk7x4k2taVDdn7EzY0k8rfdHKFWeaIJQZSugBgyaDol7YP5jBHh78OawdiSezuKRWToTnVLliSYIVfYadoOuD0Ls/2DLN7SKCOaJ65qzcNsxZvwe5+rolFI2TRDKNXo+BnXawXdjIeUwwy+PpHeLmrw8fxtbD6e4OjqlFJoglKu4e8LAaZCTCfPGIMbw8qA2BPt5Mnbmnzp/hFLlgCYI5TphjeHal2DPIlj9PqH+Xrw2JIodR0/x3/nbXB2dUlWeJgjlWh3+D5r2gQUT4NhWujcNZ+QVDfloRRyLth1zdXRKVWmaIJRriUD/t8AnCGbfDTmZjO/TjOa1Annk6/UcP5Xp6giVqrI0QSjXCwi3ksTRjbDoJXw83Zk8tB0pGTmM/3qDXvqqlItoglDlQ7M+0GEE/D4F9q+iWa1AnujbnN+2HeN/f+xzdXRKVUlOSxAiUk9EFonIFhHZLCJjHZS5TUQ2iMhGEVkhIlEFtvURke0isktEHnNWnKocueZFCKkH80ZD1mmGXx5Jj6bhvPTjVvYknHJ1dEpVOc6sQeQADxljWgCXAfeISItCZfYCPYwxrYEXgPcBRMQdmAr0BVoAwxzsqyob70AY8LZ1l/WvzyEivDK4Dd4e7jz41XpycvNcHaFSVYrTEoQx5rAxZp39PBXYCkQUKrPCGJNkv/wDqGs/7wTsMsbsMcZkATOBAc6KVZUjDbtB59Gw+j3Yu5SaQT68eGMrYg8k887i3a6OTqkqpUz6IEQkEmgHrCqi2D+A+fbzCOBAgW3xFEouBY49SkRiRCQmIUFnKKsUrnoGQi+BefdARgo3RNXhhqg6TF64k00HT7o6OqWqDKcnCBEJAGYD44wxDsdQEJFeWAni0ZIe3xjzvjEm2hgTHR4efnHBqvLByw9uehdS4uGXJwF4YUBLqgd48cCXsXqXtVJlxKkJQkQ8sZLDZ8aYOeco0waYBgwwxuRPCnAQqFegWF17naoq6nWCy++HdZ/AzgWE+HnxyuAodh47xWu/bHd1dEpVCcVKECLiLyJu9vOmItLf/vIvah8BpgNbjTGTzlGmPjAHuMMYs6PApjVAExFpKCJewFDg2+LEqiqRXk9AjRbw7f2QcZIeTcO5/bL6TFu+lz90giGlnK64NYilgI+IRAC/AHcAH51nnyvscleKSKy9XCcio0VktF1mAlAdeNveHgNgjMkB7gV+xurc/soYs7kkJ6YqAQ9vGDAVTh2BX54G4InrLqV+qB8Pz1rPqcwcFweoVOUmxblLVUTWGWPai8h9gK8x5hURiTXGtHV+iMUXHR1tYmJiXB2GKm0LnoHf34A75sElvVi7L5Eh767klo71+c/A1q6OTqkKTUTWGmOiHW0rbg1CRKQLcBvwg73OvTSCU+q8ej4G1ZvAd/dD5ik6NAjl7m6N+GL1fpbs0CvXlHKW4iaIccDjwFxjzGYRaQQscl5YShXg6QsD3oLkA7DwOQAe6N2UJjUCePTrDZxMz3ZxgEpVTsVKEMaYJcaY/saYl+3O6uPGmPudHJtSf6l/mX0D3fuwbwU+nu68dnMUCacyef67La6OTqlKqbhXMX0uIkEi4g9sAraIyCPODU2pQq56GkIawDf3QFYabeqG8K+elzB7XTwLthx1dXRKVTrFbWJqYd/kdiPW3c4Nsa5QUqrsePlD/zetsZoWvQTAfVc24dLaQTw+ZyNJp7NcHKBSlUtxE4Snfd/DjcC3xphsQAfpV2WvUQ9rFro/3ob4tXh5uPHakChOpmcx4Vu9Elqp0lTcBPEeEAf4A0tFpAHgcNgMpZyu9/MQWNtqasrJpEWdIMZe1YTv1h9i/sbDro5OqUqjuJ3UU4wxEcaY64xlH9DLybEp5ZhPEPR7AxK2wtKJAIzucQmtI4J5at4mTug0pUqViuJ2UgeLyKT8UVNF5DWs2oRSrtH0GmgzFJZPgiMb8XB3Y+KQKFIzcrSpSalSUtwmphlAKnCzvaQAHzorKKWKpc9/wDcU5v0LcrNpViuQsVc34YcNh/lRm5qUumjFTRCXGGOesSfw2blX/McAABvnSURBVGOMeQ5o5MzAlDovv1C4fiIc2QArpgDwz+6NaFPXamo6rk1NSl2U4iaIdBHpmv9CRK4A0p0TklIl0GKAtSx+GRK2n2lqOpWRw4RvNrk6OqUqtOImiNHAVBGJE5E44C3gn06LSqmSuG6iNcnQN/dCXi5Na1pNTT9uPML3Gw65OjqlKqziXsW03hgTBbQB2hhj2gFXOjUypYoroAb0eRniV8Oqd4G/mpqe1qYmpS5YiWaUM8akFJg29EEnxKPUhWlzMzTtAwtfgBO7zzQ1nc7M5cm5GynOsPZKqbNdzJSjUmpRKHWxRKDf6+DuBd/eB3l5NK0ZyIPXNOXnzUf5JlabmpQqqYtJEPqTTJUvQXWgz79h3++wZhoAd3drRPv6IUz4ZhNHUzJcHKBSFUuRCUJEUkUkxcGSCtQpoxiVKr62t8ElV8Gvz0JSHO5uwsQhUWTl5vHY7A3a1KRUCRSZIIwxgcaYIAdLoDHGo6yCVKrYRKD/FBA3+6qmPBqFB/Bon+Ys2p7AVzEHXB2hUhXGxTQxKVU+BdeFa16AuGWw1rrhf3iXSC5rFMoL328lPinNxQEqVTFoglCVU4cR0LAHLJgAyftxcxNeHRyFMYZHZ28gL0+bmpQ6H00QqnISsSYXMga+GwfGUC/Ujyeuv5Tfd53gf6v2uTpCpco9TRCq8qrWAK5+FnYvhPVfAHBrp/p0bxrOv3/cyt7jp10anlLlnSYIVbl1vAvqd4GfHoPUI4gIrwxqg5e7Gw99FUuuNjUpdU6aIFTl5uYG/d+CnEz44SEwhlrBPjw/oBXr9ifz3tLdro5QqXJLE4Sq/MIaQ8/HYdv3sHkuAAPa1qFvq1q8vmAHWw/r7LlKOaIJQlUNXe6FOu3gx0fg9AlEhBdvbEWwrycPfrWerJw8V0eoVLmjCUJVDe4eMGAqZJyEnx4FoHqAN/8Z2Iath1OYvHCHiwNUqvzRBKGqjpotofvDsHEWbPkWgN4tajK4Q13eWbybdfuTXBygUuWLJghVtXR9EOq0t4bhSNwDwIQbWlA72JdxM2NJzch2cYBKlR+aIFTV4uEFQz6yBqufNQKyMwjy8WTy0LbEJ6Ux4ZvNLg5QqfJDE4Sqeqo1gJveg8Pr4efHAYiODGXsVU2Z++dB5qyLd3GASpUPmiBU1dSsL1wxFmJmwIZZANx7ZWM6RYby9LxNxOld1kppglBV2JVPW3dZfzcWErbj7ia8PrQtHu5ujJ35p176qqo8pyUIEaknIotEZIuIbBaRsQ7KNBeRlSKSKSIPF9oWJyIbRSRWRGKcFaeqwtw9YfAM8PSFr4ZD1mkiQnx5eVBr1sefZNICvfRVVW3OrEHkAA8ZY1oAlwH3iEiLQmUSgfuBiec4Ri9jTFtjTLQT41RVWVAdGPQBJGyDX54CoE+r2gzrVJ/3lu5m+c7jLg5QKddxWoIwxhw2xqyzn6cCW4GIQmWOGWPWAHptoXKdS66ELvdY/RFxywGY0K8Fl4QH8MBXsSSkZro4QKVco0z6IEQkEmgHrCrBbgb4RUTWisioIo49SkRiRCQmISHh4gJVVVevJ6FaJHx7H2Sn4+vlztRb25OSns0DX8bqBEOqSnJ6ghCRAGA2MM4YU5JR0boaY9oDfbGap7o7KmSMed8YE22MiQ4PDy+FiFWV5OVnTTCUuAcW/weAZrUCea5/S5bvOs7bi3e5OEClyp5TE4SIeGIlh8+MMXNKsq8x5qD9eAyYC3Qq/QiVKqBhd2g/HFa8CQfXAXBLx3r0j6rDpAU7WLXnhIsDVKpsOfMqJgGmA1uNMZNKuK+/iATmPweuATaVfpRKFdL7eQioaTU15WYjIvx7YGvqh/px/8w/OXFK+yNU1eHMGsQVwB3AlfalqrEicp2IjBaR0QAiUktE4oEHgadEJF5EgoCawHIRWQ+sBn4wxvzkxFiVsviGwPWT4Ogm+P0NAAK8PXjr1vYknc7moVnrtT9CVRkezjqwMWY51og3RZU5AtR1sCkFiHJGXEqdV/ProOVAWPIKXNofwpvRKiKYp/tdytPfbOb9ZXsY3eMSV0eplNPpndRKOdL3FfAKgDl3Q3YGALdf1oDrWtfilZ+26f0RqkrQBKGUIwHhcOPb1oB+9gRDIsIrg6O4JDyAe79Yx4HENBcHqZRzaYJQ6lya9YWuD8DajyD2C8Dqj/jgzmjy8gx3fxJDWlaOa2NUyok0QShVlF5PQWQ3+P4BOGrNFREZ5s+bt7Znx9FUHpm1AWO001pVTpoglCqKu4c1oJ9PMHx5hzWnNdCjaTjj+zTnh42HeWfJbhcHqZRzaIJQ6nwCasCQDyEpDr65B+wawz+7N+KGqDq8+vN2Fm0/5toYlXICTRBKFUeDy6H3c7D1O1g5FbA7rQe14dJaQdz/+Z9sP5Lq4iCVKl2aIJQqri73wqU3wIIJEPc7AL5e7nwwPBo/b3dGfLiaIyczXBykUqVHE4RSxSUCA962Rn39+v8g9QgAESG+zBjRkdSMHEZ8uJqUDB29XlUOmiCUKgmfILjlf5CZCrP+D3KtZNCyTjDv3N6eXcdOMfrTtTpdqaoUNEEoVVI1W8ANU2D/Cvj12TOruzUJ5+VBbVix+wTjv16vl7+qCs9pYzEpVam1GQLxq2HlW1A3GlreBMCgDnU5fDKdib/soHaIL4/2ae7iQJW6cJoglLpQ17wEh/6Eb+6FGi0gvBkA9/RqzMHkDN5ZvJsQX0/+qQP7qQpKm5iUulAeXjDkY/DwgS9vh7REwLr89cUbW3FDVB3+M38b05fvdXGgSl0YTRBKXYzgCLj5Y+smus+GQOYpANzdhEk3R9G3VS1e+H4Ln6yMc2WUSl0QTRBKXazIrjD4Qzi0zqpJ5Fizznm6uzF5aDuuvrQmE77ZzBer97s4UKVKRhOEUqXh0n7Q/y3Yswhm3wV5uQB4ebgx9bZ29GoWzhNzNzIr5oCLA1Wq+DRBKFVa2t0G1/4btn4L3409M2aTt4c779zega6Nwxg/ewMztSahKghNEEqVpi73QPdH4M9PrSE57CTh4+nO+3dE071JOI/N2cjURbv0PglV7mmCUKq09XoSOt4FK6bAby+cSRK+Xu5MGx7NjW2tEWCf/34LeXmaJFT5pfdBKFXaRKDvq5CXA8tes4bj6P08iODp7sakm9sS6u/NjN/3kng6i1cHR+Hlob/VVPmjCUIpZ3Bzg+tfBzcPqyaRl2P1T4jg5iY83e9SwgK9eOWn7SSnZfPO7e3x89L/jqp80Z8tSjmLmxtcNxE6j4E/3oYfH4E8axA/EeFfPRvz34GtWbYzgRun/s7uhFMuDlips2mCUMqZRKDPf+Dy+2HNB/DDA2eSBMDQTvX5eGQnjp/Kov+by/lu/SEXBqvU2TRBKOVsIlYfRLeHYO1HMHskZP81sVC3JuH8cH9XmtcO4r4v/uSZbzaRmZPruniVsmmCUKosiMCVT0PvF2DzXPhkwJmxmwBqB/syc9Rl3NW1IR+v3MfN767kQGKaCwNWShOEUmVHBK64H4Z8ZI0CO+1qSNxzZrOnuxtP9WvBu7e3Z0/CafpOXsbM1fv1fgnlMpoglCprLW+C4d9CehJM6w0H1py1uU+r2vw4thutI4J5bM5Ghn+4hsMn010UrKrKNEEo5Qr1L4N/LADvAPi4H8R+ceaGOoB6oX58dldnnuvfkjV7E7nm9aV8vTZeaxOqTGmCUMpVwhrDXQshogPMGw2zhp/VL+HmJgy/PJL5Y7vRvFYgD89az4gP17DrWKoLg1ZViSYIpVzJPwyGfwdXPwvbfoS3u8CuX88qEhnmz5ejujChXwvW7Uvi2jeWMeGbTSSeznJJyKrqkMpUZY2OjjYxMTGuDkOpC3N4PcwZBQnboNMouPo58PI7q8iJU5m88etOPl+9Hz8vd8Ze1YQ7u0TqUB3qgonIWmNMtMNtmiCUKkeyM2Dhc9ad19Ubw43vQL1Ofyu282gqL/24lcXbE6gX6suYHo0Z1CECbw93FwStKjJNEEpVNHuWwDf3Qko8dLnXGiHW0+dvxZbsSGDSgh2sP5BMrSAfRnVvxLBO9fH10kShiqeoBOG0eqmI1BORRSKyRUQ2i8hYB2Wai8hKEckUkYcLbesjIttFZJeIPOasOJUqlxr1gDG/Q/s7rcH+3usO8Wv/VqxH03Dm/etyPv1HJxpU9+P577fQ9eXfmLpol/ZRqIvmtBqEiNQGahtj1olIILAWuNEYs6VAmRpAA+BGIMkYM9Fe7w7sAHoD8cAaYFjBfR3RGoSqlHYthG/vg9TDVm2ix3jwDnRYdE1cIm/9toslOxLw8nDjhjZ1uLNLA6LqhZRx0KqicEkNwhhz2Bizzn6eCmwFIgqVOWaMWQNkF9q9E7DLGLPHGJMFzAQGOCtWpcq1xlfBv1ZC29us2sSbHeDPz84a9C9fx8hQPh7ZiZ/HdeeW6Hr8tOkwA6b+zoC3ljMr5gCpGYX/qyl1bmXSByEikcBSoJUxJsXB9meBUwVqEIOBPsaYu+zXdwCdjTH3Oth3FDAKoH79+h327dvnpLNQqhyIXwvzx8PBGKjTHvq+AvU6nrN4akY2c/88yCcr97Hr2Cm83N3o2iSMPi1rcXWLmoT6e5Vh8Ko8cmkntYgEAEuAl4wxc85R5lkuMEEUpE1MqkrIy4ONX8GCZ+DUEWg5EC77F9SNtsZ7csAYw9p9Sfy06Qg/bT5CfFI6bgKdGoZybcta9G5Rk7rV/Bzuqyo3lyUIEfEEvgd+NsZMKqLcs5ydILoAzxpjrrVfPw5gjPlPUe+nCUJVKZmnYPkkWPU+ZKVCrdbWXNith4CX/zl3M8aw+VDKmWSx65g1UVHLOkFc06IW17SsSfNagcg5ko2qXFySIMT66/oYSDTGjDtP2Wc5O0F4YHVSXwUcxOqkvtUYs7mo42iCUFVSZips+ArWTIdjm8E7CKKGQptbrGE8zvNFvyfhFAu2HOWXLUdZtz8JYyAswJsODUKIbhBK+wbVaBURpPdYVFKuShBdgWXARiC/N+0JoD6AMeZdEakFxABBdplTQAtjTIqIXAe8AbgDM4wxL53vPTVBqCrNGNj/B8RMhy3fQG4WhDSAVoOg9WCo0eK8ySIhNZPfth1l1Z5EYvYlsd+ek8LLw43WEcG0rx9Cu/rVaFc/hNrBvmVxVsrJ9EY5paqa9GTY9gNs+tq66c7kQnhzaN4Pml9ndXAXownpWGoG6/YlEROXxJ8Hktl48CRZOdbvvdrBPrSpG0zzWkE0rxVIs1qBNKjuj7ubNk1VJJoglKrKTiXA1m9g01zYvwJMHgTWgWZ9rWQR2Q08vIt1qKycPLYeTmHd/iTW7U9m88GTxJ04TZ79NeLj6UaTGoE0qRlA05qBNK0ZQJMagUSE+OKmiaNc0gShlLKkJcKOn2H7D7DrN8g+DV4BcMmVVsJoco01wmwJZGTnsvPoKbYdSWHbkVR2HLWWoymZZ8r4ebnTMMyfyDB/GoX509Be6of6EervpR3iLqQJQin1d9kZsHcJbJ8PO36y7tRGrMEB63aE0EZQ/RIIvQSCIsCtZPfVnkzLZscxK1nsPHqKuBOn2Xv8NAcS087UOAD8vdypF+pH3Wp+1A/1o16or/3oR71qfjqulJNpglBKFc0YOBwL23+CnT/Dsa2Qk/HXdndvqNEc6naCep2tm/NCGhSrH6OwrJw8DiSlsTfhNPsT0ziQlMaBxHQO2M/TsnLPKh8W4E29UF/qVfOjbjVfO5n4EhHiS61gH/y8PC727Ks0TRBKqZLJy4PUQ5C4B07shhO74MgG607u7NNWGf8a0OByaNoHmvQucdOUI8YYEk9nsT8xjf2JacQnpbPvxGnik9KJT0rnUHI6OXlnf2cF+nhQK8iHWsE+1AzyOZM8IqpZSaVWsA+e7jpfxrloglBKlY7cHDi2BQ6sgvg11hVSp44AYjVLNesDjXpBtUjwrXZBNYyi5OTmcTQ1k/jENA6dTOfIyUyOnEznSEoGR1IyOZyczrHUzLP2cROoEehD7RAf6gT7UjvYh9ohvtQK8qFGkDc1A61HH8+q2ZSlCUIp5Rx5eXBkvd3xPd9qpsrn6QfBda3+i5B6Vl9G9cYQ1sRKIMW8cqqkMnNyOZycwcHkdA4mpROfnM7h5HQOnUw/sz4z5+8DHQb5eFAnxPdM30d+s1btEB9qBPoQ6u9VKS/h1QShlCobKYchfjWcjIeTB+HkAUg5CMn74XTCX+XEzerDqNUKare1lygICLe2GwOZKXD6uLVfQA2r07wUGGNISsvmaEoGx1IzOWY/Hk3J4GBSOgeSrOatjOyzk4i7m1Dd34saQd7UCvKhTshfTVn5j+EB3hXuiixNEEop10tPhsTdVp/G8Z1wfIfVr5G4568ygbWt5HE6wboTvKBqDaHx1dYS2RW8A5wWqjGG46eyOJCUxtGTVgJJSM3kWKr1/MhJqyaSmpFz1n4+nm527cOPenaHekSIL7VDfKkT4kOYv3e5ux9EE4RSqvzKOAlHNsKhWOvRzd3q8PavAf7h4F8dEvfCrl9h71LITgM3T6jTDsKbQvUmENb0r6Yrd88yCz0lI5tD+U1ZSX9dibU/MZ34xDRSM89OIF7ubtQOsTrS64ZYV2PVtZuy6oX6ER5Q9glEE4RSqnLIyYT9K61kcfBPqxZy+thf2908rCRRvYl1D0f1xhDa0JqBz9MfvPysR+8Ap/WB5DPGcDI9m0PJGRyy+0Dy+0UOJlsJJaFQh7q3hxv1Qv1oYN8HUrear71YNZEQP89Sb8LSBKGUqrzSk63LcPObrU7sspqxEneffS9HYUF1IbyZNUZVeDNrCYqw+jucnDzyZWTnWjWPpDTi7Ut7rSWd/SdOc7rQPSH+Xu5EVPM90/9RJ8T3zGW90ZGhFxSDJgilVNWTlwcp8ZC0D7JOQ9Ypq3kqK83qAD+xCxK2QcIOyEk/e1/vYKuZK6AGBNS0EkdQHXuJgOAICKgF7s67Sc8YQ3Ja9pnaRnxS2pkayKGT1mNSmjWFbFiAFzFP9b6g9ykqQegtiEqpysnNDULqW0tR8vLg5H4rUaQetpqsTiVYHeWnE+DoJtj5i5VcChJ3K2EE17WWgJrg6WvVPjx8wdPHmpsjvLm1eJRselcRoZq/F9X8vWgVEeywTFpWDoeS0zmZ7py5xjVBKKWqNjc3q9+iWuS5yxgDGcmQcsi6fDfloH0pr70cWG0lk+x0wEGrjJun1YRVsxXUbAmBtcAvFPzCwK+6tXiVfMpXPy8PGtcILPF+xaUJQimlzkfEujPct5r1BX8uxliX5+ZkWIMhpifC0c3W1VlHN8GexbBhpuN9A2paV2Pl94uENbU62APrOLUpqyiaIJRSqrSI2E1M3uATDIE1ocal1ox++dKTrBsA005YS/7NgIl7IGG7NX1sZkqBY+Y3ZdWz7kgPaWCPstvIujvdL7TUhzTJpwlCKaXKUn5NhCaOtxsDqUfg+Harg/3kAUg+YD3uWwkbZ1mTPuXzDrZqNf/3Y6knCk0QSilVnohAUG1rcSQnC5L3/TXSbuIeq1nLCbUITRBKKVWReHhZd42HnaMGUop0kHSllFIOaYJQSinlkCYIpZRSDmmCUEop5ZAmCKWUUg5pglBKKeWQJgillFIOaYJQSinlUKWaD0JEEoB9F7h7GHC8FMNxpcp0LqDnU55VpnOBynU+xT2XBsaYcEcbKlWCuBgiEnOuSTMqmsp0LqDnU55VpnOBynU+pXEu2sSklFLKIU0QSimlHNIE8Zf3XR1AKapM5wJ6PuVZZToXqFznc9Hnon0QSimlHNIahFJKKYc0QSillHKoyicIEekjIttFZJeIPObqeEpKRGaIyDER2VRgXaiILBCRnfZjNVfGWFwiUk9EFonIFhHZLCJj7fUV9Xx8RGS1iKy3z+c5e31DEVll/819KSJero61uETEXUT+FJHv7dcV+VziRGSjiMSKSIy9rkL+rQGISIiIfC0i20Rkq4h0udjzqdIJQkTcgalAX6AFMExEWrg2qhL7COhTaN1jwEJjTBNgof26IsgBHjLGtAAuA+6x/z0q6vlkAlcaY6KAtkAfEbkMeBl43RjTGEgC/uHCGEtqLLC1wOuKfC4AvYwxbQvcL1BR/9YAJgM/GWOaA1FY/04Xdz7GmCq7AF2Anwu8fhx43NVxXcB5RAKbCrzeDtS2n9cGtrs6xgs8r2+A3pXhfAA/YB3QGevuVg97/Vl/g+V5AeraXzJXAt8DUlHPxY43DggrtK5C/q0BwcBe7AuPSut8qnQNAogADhR4HW+vq+hqGmMO28+PADVdGcyFEJFIoB2wigp8PnaTTCxwDFgA7AaSjTE5dpGK9Df3BjAeyLNfV6finguAAX4RkbUiMspeV1H/1hoCCcCHdhPgNBHx5yLPp6oniErPWD8dKtS1zCISAMwGxhljUgpuq2jnY4zJNca0xfr13Qlo7uKQLoiI9AOOGWPWujqWUtTVGNMeq4n5HhHpXnBjBftb8wDaA+8YY9oBpynUnHQh51PVE8RBoF6B13XtdRXdURGpDWA/HnNxPMUmIp5YyeEzY8wce3WFPZ98xphkYBFWM0yIiHjYmyrK39wVQH8RiQNmYjUzTaZingsAxpiD9uMxYC5WAq+of2vxQLwxZpX9+mushHFR51PVE8QaoIl9JYYXMBT41sUxlYZvgeH28+FYbfnlnogIMB3YaoyZVGBTRT2fcBEJsZ/7YvWnbMVKFIPtYhXifIwxjxtj6hpjIrH+n/xmjLmNCnguACLiLyKB+c+Ba4BNVNC/NWPMEeCAiDSzV10FbOFiz8fVnSuuXoDrgB1YbcNPujqeC4j/C+AwkI31K+IfWG3DC4GdwK9AqKvjLOa5dMWqAm8AYu3lugp8Pm2AP+3z2QRMsNc3AlYDu4BZgLerYy3hefUEvq/I52LHvd5eNuf/36+of2t27G2BGPvvbR5Q7WLPR4faUEop5VBVb2JSSil1DpoglFJKOaQJQimllEOaIJRSSjmkCUIppZRDmiCUKgERybVH/8xfSm0wNxGJLDgqr1Ku5nH+IkqpAtKNNXSGUpWe1iCUKgX23AKv2PMLrBaRxvb6SBH5TUQ2iMhCEalvr68pInPtuSLWi8jl9qHcReQDe/6IX+w7sJVyCU0QSpWMb6EmplsKbDtpjGkNvIU18inAm8DHxpg2wGfAFHv9FGCJseaKaI91Ny9AE2CqMaYlkAwMcvL5KHVOeie1UiUgIqeMMQEO1sdhTQ60xx5w8IgxprqIHMcajz/bXn/YGBMmIglAXWNMZoFjRAILjDW5CyLyKOBpjHnR+Wem1N9pDUKp0mPO8bwkMgs8z0X7CZULaYJQqvTcUuBxpf18BdbopwC3Acvs5wuBMXBmUqHgsgpSqeLSXydKlYyvPUNcvp+MMfmXulYTkQ1YtYBh9rr7sGb5egRrxq//s9ePBd4XkX9g1RTGYI3Kq1S5oX0QSpUCuw8i2hhz3NWxKFVatIlJKaWUQ1qDUEop5ZDWIJRSSjmkCUIppZRDmiCUUko5pAlCKaWUQ5oglFJKOfT/35SdHrqV44kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hluN6UIbNyj7"
      },
      "source": [
        "Follow the assignment handout for questions to be answered in this part of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8LXS4oHZA9M"
      },
      "source": [
        "class ConvNet2(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 32, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(32, 128, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(128, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "    x = objax.functional.max_pool_2d(self.conv_2(x), 2, 2)\n",
        "   \n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "model2 = ConvNet2()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0NJdLRriaeD"
      },
      "source": [
        "#Define loss function as averaged value of of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = model2(x)\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(model2(x)), model2.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, model2.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in model2.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    print(\"flags\")\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    count=0\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      params.value -= lr * grad\n",
        "      \n",
        "      ####################\n",
        "    \n",
        "        \n",
        "      \n",
        "      #PUT YOUR CODE HERE#\n",
        "      ####################                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh-_hx_ns0ES"
      },
      "source": [
        "#Problem 2 part 7, testing my selected model2\n",
        "def train_last(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  avg_test_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "  test_acc_epoch = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss =  0\n",
        "      avg_test_loss =  0   # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0 \n",
        "      test_acc=0       # validation accuracy per batch\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          batch =train_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "      # run validation\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "          batch = val_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      # run vtest\n",
        "      test_indices = np.arange(len(X_test)) \n",
        "      np.random.shuffle(test_indices)    \n",
        "      for it in range(0, X_test.shape[0], BATCH):\n",
        "          batch = test_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_test_loss += float(loss_function(X_test[batch], Y_test[batch])) * len(batch)\n",
        "          test_prediction = predict(X_test[batch]).argmax(1)\n",
        "          test_acc += (np.array(test_prediction).flatten() == Y_test[batch]).sum()\n",
        "      test_acc_epoch.append(test_acc/X_test.shape[0])\n",
        "      avg_test_loss_epoch.append(avg_test_loss/X_test.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Test Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f Test Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0],avg_test_loss/X_test.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0],100*test_acc/X_test.shape[0]))\n",
        "  \n",
        "  #Plot training loss\n",
        "  plt.title(\"Train Validation Test Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.plot(avg_test_loss_epoch, label=\"Test\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train Validation Test Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.plot(test_acc_epoch, label=\"Test\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ABKxImijqh",
        "outputId": "9835c256-67aa-4b39-e5fa-410b44e32b91"
      },
      "source": [
        "train_last(EPOCHS = 30, BATCH = 64, LEARNING_RATE = 1e-1) #before running this part please reset \n",
        "#to get rid of variables and only run from conv2 model definition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flags\n",
            "Epoch 0001  Training Loss 2.13 Validation Loss 2.10 Test Loss 2.12 Training Accuracy 25.07 Validation Accuracy 21.22 Test Accuracy 21.43\n",
            "Epoch 0002  Training Loss 1.96 Validation Loss 1.89 Test Loss 1.91 Training Accuracy 32.75 Validation Accuracy 28.35 Test Accuracy 27.27\n",
            "Epoch 0003  Training Loss 1.85 Validation Loss 1.75 Test Loss 1.77 Training Accuracy 37.25 Validation Accuracy 35.72 Test Accuracy 33.48\n",
            "Epoch 0004  Training Loss 1.76 Validation Loss 1.72 Test Loss 1.72 Training Accuracy 40.11 Validation Accuracy 36.48 Test Accuracy 36.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-61jYOgXC5H"
      },
      "source": [
        "class ConvNet3(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 64, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(64, 512, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(512, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "    x = self.conv_2(x)\n",
        "  \n",
        "   \n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "model3 = ConvNet3()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy0kkVaqXJrn"
      },
      "source": [
        "#Define loss function as averaged value of of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = model3(x)\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(model3(x)), model3.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, model3.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in model3.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    print(\"flag3\")\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    count=0\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      params.value -= lr * grad\n",
        "      \n",
        "      ####################\n",
        "    \n",
        "        \n",
        "      \n",
        "      #PUT YOUR CODE HERE#\n",
        "      ####################                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDdpo-EqkvKt"
      },
      "source": [
        "train(EPOCHS = 60, BATCH = 16, LEARNING_RATE = 1e-2) #similar to above, reset the parameters and run only the part after conv3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XOXmJiGZF78"
      },
      "source": [
        "You have now completed Part 2 of the assignment. Good job!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cno9fjO1o2Jq"
      },
      "source": [
        "##**Part 3. Trying Out a New Dataset**\n",
        "\n",
        "See the handout for instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn4WCyKIZ4iH",
        "outputId": "a6253dd4-50a3-45a5-a361-1adadb06a341"
      },
      "source": [
        "import tensorflow as tf\n",
        "X_train=[]\n",
        "Y_train=[]\n",
        "X_test=[]\n",
        "Y_test=[]\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.fashion_mnist.load_data() #picked fashion_mnist dataset,gray coded\n",
        "X_train = X_train[:,None,:,:] / 255.0\n",
        "Y_train = Y_train.flatten()\n",
        "X_test = X_test[:,None,:,:] / 255.0\n",
        "Y_test = Y_test.flatten()\n",
        "\n",
        "np.random.seed(1)\n",
        "# To create a validation set, we first concate the original splitted dataset into a single dataset \n",
        "# then randomly shuffle the images and labels in the same way (seed = 1)\n",
        "X_data = np.concatenate([X_train, X_test], axis = 0)\n",
        "print(X_data.shape) #in total 70000 examples\n",
        "Y_data = np.concatenate([Y_train, Y_test], axis = 0)\n",
        "\n",
        "N = np.arange(len(X_data))\n",
        "np.random.shuffle(N)\n",
        "X_data = X_data[N]\n",
        "Y_data = Y_data[N]\n",
        "\n",
        "#Next, we partition the randomly shuffled dataset into training, validation and testset according a ratio\n",
        "train_ratio = 0.80\n",
        "valid_ratio = 0.1\n",
        "n_train = int(len(X_data) * train_ratio)\n",
        "n_valid = int(len(X_data) * valid_ratio)\n",
        "\n",
        "X_train, X_valid, X_test = X_data[:n_train], X_data[n_train:n_train+n_valid], X_data[n_train+n_valid:]\n",
        "Y_train, Y_valid, Y_test = Y_data[:n_train], Y_data[n_train:n_train+n_valid], Y_data[n_train+n_valid:]\n",
        "\n",
        "print(X_train.shape) #56000 training examples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "(70000, 1, 28, 28)\n",
            "(56000, 1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvBUIa31B6Gz"
      },
      "source": [
        "class ConvNet4(objax.Module): #This is the model that I picked \n",
        "  def __init__(self, number_of_channels = 1, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(1, 8, 3), objax.functional.relu])\n",
        "    \n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(8, 32, 2), objax.functional.relu])\n",
        "    self.conv_3 = objax.nn.Sequential([objax.nn.Conv2D(32, 64, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(64, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "    \n",
        "    x=objax.functional.max_pool_2d(self.conv_2(x), 2, 2)\n",
        "    x=objax.functional.max_pool_2d(self.conv_3(x), 2, 2)\n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "model4 = ConvNet4()\n",
        "#print(model.vars())\n",
        "#You can examine the architecture of our CNN by calling model.vars()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHE3TEWMCfKF"
      },
      "source": [
        "#Define loss function as averaged value of of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = model4(x)\n",
        "   \n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(model4(x)), model4.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, model4.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in model4.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    count=0\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      params.value -= lr * grad\n",
        "      \n",
        "      ####################\n",
        "    \n",
        "        \n",
        "      \n",
        "      #PUT YOUR CODE HERE#\n",
        "      ####################                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYSyuw91EHMC"
      },
      "source": [
        "def train(EPOCHS = 30, BATCH = 32, LEARNING_RATE = 1e-2):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss =  0  # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0        # validation accuracy per batch\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          batch =train_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "      # run validation\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "          batch = val_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))\n",
        "  \n",
        "  #Plot training loss\n",
        "  plt.title(\"Train vs Validation Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train vs Validation Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZZwbiQf8yst"
      },
      "source": [
        "train_last(EPOCHS = 30, BATCH = 16, LEARNING_RATE = 1e-1) #Q3.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eye-uBnQWgc8"
      },
      "source": [
        "##**Problem 4. Open-Ended Exploration**\n",
        "\n",
        "See the handout for instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXIW2gwQg-nS"
      },
      "source": [
        "#.load_data() by default returns a split between training and test set. \n",
        "# We then adjust the training set into a format that can be accepted by our CNN\n",
        "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "print(X_train.shape)\n",
        "X_train = X_train.transpose(0, 3, 1, 2) / 255.0\n",
        "print(X_train.shape)\n",
        "Y_train = Y_train.flatten()\n",
        "X_test = X_test.transpose(0, 3, 1, 2) / 255.0\n",
        "Y_test = Y_test.flatten()\n",
        "\n",
        "np.random.seed(1)\n",
        "# To create a validation set, we first concate the original splitted dataset into a single dataset \n",
        "# then randomly shuffle the images and labels in the same way (seed = 1)\n",
        "X_data = np.concatenate([X_train, X_test], axis = 0)\n",
        "Y_data = np.concatenate([Y_train, Y_test], axis = 0)\n",
        "\n",
        "N = np.arange(len(X_data))\n",
        "np.random.shuffle(N)\n",
        "X_data = X_data[N]\n",
        "Y_data = Y_data[N]\n",
        "\n",
        "#Next, we partition the randomly shuffled dataset into training, validation and testset according a ratio\n",
        "train_ratio = 0.80\n",
        "valid_ratio = 0.1\n",
        "n_train = int(len(X_data) * train_ratio)\n",
        "n_valid = int(len(X_data) * valid_ratio)\n",
        "\n",
        "X_train, X_valid, X_test = X_data[:n_train], X_data[n_train:n_train+n_valid], X_data[n_train+n_valid:]\n",
        "Y_train, Y_valid, Y_test = Y_data[:n_train], Y_data[n_train:n_train+n_valid], Y_data[n_train+n_valid:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPDruT0ThB1t"
      },
      "source": [
        "class ConvNet2(objax.Module): #I am using the same model that I picked in Part 2 -model2\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 32, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(32, 64, 2), objax.functional.relu])\n",
        "    self.conv_3 = objax.nn.Sequential([objax.nn.Conv2D(64, 128, 2), objax.functional.relu])\n",
        "   # self.conv_4 = objax.nn.Sequential([objax.nn.Conv2D(64, 128, 2), objax.functional.relu]) #I tried to add this layer but did not improve so much\n",
        "\n",
        "    self.linear = objax.nn.Linear(128, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2) #max pooling\n",
        "    x = objax.functional.max_pool_2d(self.conv_2(x), 2, 2) #max pooling\n",
        "    x = objax.functional.average_pool_2d(self.conv_3(x), 2, 2) #average pooling\n",
        "    #x = objax.functional.average_pool_2d(self.conv_4(x), 2, 2)\n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "model2 = ConvNet2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZZSd3mKhFmB"
      },
      "source": [
        "#Define loss function as averaged value of of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = model2(x)\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(model2(x)), model2.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, model2.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in model2.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    count=0\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      \n",
        "      \n",
        "      ####################\n",
        "      params.value -= lr * grad\n",
        "      ####################                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9ad1zmVhNDl"
      },
      "source": [
        "#Problem 2 part 7, testing my selected model2\n",
        "def train_last(EPOCHS = 30, BATCH = 64, LEARNING_RATE = 1e-1):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  avg_test_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "  test_acc_epoch = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss =  0\n",
        "      avg_test_loss =  0   # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0 \n",
        "      test_acc=0       # validation accuracy per batch\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          batch =train_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "      # run validation\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "          batch = val_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      # run vtest\n",
        "      test_indices = np.arange(len(X_test)) \n",
        "      np.random.shuffle(test_indices)    \n",
        "      for it in range(0, X_test.shape[0], BATCH):\n",
        "          batch = test_indices[it:it+BATCH] #PUT YOUR CODE HERE#\n",
        "          avg_test_loss += float(loss_function(X_test[batch], Y_test[batch])) * len(batch)\n",
        "          test_prediction = predict(X_test[batch]).argmax(1)\n",
        "          test_acc += (np.array(test_prediction).flatten() == Y_test[batch]).sum()\n",
        "      test_acc_epoch.append(test_acc/X_test.shape[0])\n",
        "      avg_test_loss_epoch.append(avg_test_loss/X_test.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Test Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f Test Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0],avg_test_loss/X_test.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0],100*test_acc/X_test.shape[0]))\n",
        "  \n",
        "  #Plot training loss\n",
        "  plt.title(\"Train Validation Test Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.plot(avg_test_loss_epoch, label=\"Test\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train Validation Test Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.plot(test_acc_epoch, label=\"Test\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hFjmp8qhQHS"
      },
      "source": [
        "train_last(EPOCHS = 30, BATCH = 64, LEARNING_RATE = e-1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
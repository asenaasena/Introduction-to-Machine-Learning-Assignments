# -*- coding: utf-8 -*-
"""ml-assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HV_lx5zrZotR927R6s-zlh8rU0_CyqZ7
"""

############################################################################
#################Introduction to ML Assignment-1(k-means clustering)########
########Zeliha Asena Kırık##################################################

#This project takes in UCI ML Breast Cancer Wisconsin dataset
#Implements k-means clustering and apply to data
#Plots the cost function with respect to number of clusters(K)

import numpy as np
import matplotlib.pyplot as plt

#import breast_cancer data
from sklearn.datasets import load_breast_cancer

dataset= load_breast_cancer()
arr=dataset.data
#dataset consists of 569 training examples with 30 features
#where each row denotes a training example.

###############################################################################
#Function Definitions
#################################################################################


def MinMaxNormalize(array):
    m,n = array.shape
   
    scale=np.amax(array, axis=0)-np.amin(array, axis=0)
    array=array-np.ones((m,n))*np.amin(array, axis=0)
    array=np.divide(array,scale)
    
    return array

def ClosestCentroids(array, centroids):

    #Takes in data points and centroids as input.
    #Returns the closest centroids in form of index

    K = centroids.shape[0] #number of clusters(K)
    index = np.zeros((array.shape[0],1)) #it is a vector, showing assignments of points to closest centroids
    temp = np.zeros((centroids.shape[0],1))
    
    for i in range(array.shape[0]): #number of training examples
        for j in range(K): 
            distance = array[i,:] - centroids[j,:]
            length = np.dot(distance,distance)
            temp[j] = length #for every training example calculate the distance of points to the K number of centroids
        index[i] = np.argmin(temp)+1 
    return index


def ComputeCentroids(array, index, K):
    
    # returns the updated centroid points by computing the means of the data points assigned to each centroid.
    
    #initializations
    m, n = array.shape[0],array.shape[1]
    centroids = np.zeros((K,n)) #K clusters with dimension n
    count = np.zeros((K,1)) #for computing the mean 
    
    for i in range(m):
        indexa = int((index[i]-1))
        centroids[indexa,:]+=array[i,:]
        count[indexa]+=1
    
    return centroids/count


def ErrorFunct( array, centroids, index):
    
    #returns the error value by computing squared distance of the data points to corresponding centroid

    myerror=0 
    
    m, n = array.shape
    
    for i in range(m): 
        dist=array[i,:]-centroids[int(index[i])-1,:]
        myerror+=np.dot(dist,dist) #distance squared(by using dot product)
    
    return ((myerror)/m)


def iterate(array, centroids, index, K):
    
    #Takes initial centroids and dataset, employ ComputeCentroids and ClosestCentroids
    #to find mean of points and change the position of centroids
    #Calculate error function that is mean of sqaured distances of points to the related centroids
    #Apply these operation consequently until cost function converges
    #Since there is problem of convergence to local minimum,100 random initializations are tried and
    #cost functions are compared
    #Global minimum is the cost corresponding to minimum of these local minimums
    
    cost_memo=errorfunct(array,centroids,index)
    cost=-1 

    while cost<cost_memo:  #convergence condition, we know that cost is monotonically decreasing function 

        cost_memo=cost
        # compute the centroids' mean
        centroids = ComputeCentroids(array, index, K)
        # assign each training example to the nearest centroid
        index = ClosestCentroids(array, centroids)
        #compute the squared distance
        cost=ErrorFunct(array,centroids,index)
        #print("it converges")
        
    return (index,centroids,cost)

def KClustering(array, K):

    #This is the function expected for the Problem 1.1
    #Implementation of k-means clustering
    #Function takes dataset and number of clusters as input, and returns final index and  final centroids
    #points where cost function converges to global minimum 

    #centroids are initialized randomly choosing K points from dataset
    initial_centroids = array[np.random.choice(arr.shape[0], K),:]
    #instead of randomly selecting, another selection could be implemented as
    #initial_centroids = arr[300:K+300,:] 
    index = ClosestCentroids(array, initial_centroids)
   
    (index,centroids,cost)=iterate(array, initial_centroids, index, K) 
    #print(cost)
    cost_init=cost
  
    for i in range (100): #randomly initialize centroids 100 times to avoid stucking in local minimum
     
      initial_centroids = array[np.random.choice(arr.shape[0], K),:]
      #initial_centroids = arr[300:K+300,:] 
      index = ClosestCentroids(array, initial_centroids)
      (index,centroids,cost)=iterate(array, initial_centroids, index, K)

      if cost<cost_init:
         cost_init=cost
         index_memo=index
         centroids_memo=centroids
         #print("less error is detected",cost)
         

    return (index_memo,centroids_memo)

###############################################################
################This is the second part of Problem1############
###############################################################

###########This is not necessary, but if we would like to limit our dataset
#into the range [0,1] we should normalize, this method helps if our features
#have significantly different values

arr=MinMaxNormalize(arr)

cost_tot=np.zeros((6,1))

#computing cost for different number of clusters

for K in range (2,8):

   (index,centroids)=KClustering(arr, K)
   cost_tot[K-2]=ErrorFunct(arr, centroids, index)

##################################################################################
#Plotting cost(error) function for different number of cluster ranging from 2 to 7
##################################################################################
#print(cost_tot)

x_data = list(range(2,8))
plt.plot(x_data, cost_tot)
plt.ylabel('Cost Function')
plt.xlabel('Number of Clusters')
plt.title('K-means Clustering Algorithm(Min-Max Normalization Applied)')